## 类型
- **监督学习**（surpervised learning）：有X,y; 分类（Classification离散目标）、回归（Regression拟合连续目标）、排名（Ranking）
- **非监督学习**（unsurpervised learning）：有X无y；聚类（Clustering）、去噪（自编码器Auto Encoding）
- **半监督学习**（semi-surpervised learning）：打了一部分标签
- **增强学习**（reinforcement learning）：agent和环境的不断交互，进行奖励
> 监督不监督的区别在于是否是哟label

> 分类和回归的区别在于是否枚举值(离散值)

> 生成模型(先要算一遍联合概率)有：Naive Bayes，HMM； 判别模型有：Perceptron，KNN，DT，LR，SVM，emsemble，NN（神经网）,CRF(条件场)。

## 学习步骤
+ 数据获取与清洗 Data Cleaning: 升维，ETL
+ 特征提取 Feature Extraction: 升维，缺失值填充，归一化(Z-score/Min-Max Normalization)
+ 特征选择 Feature Selection: 降维，线性相关(皮尔逊系数)，PCA，model.feature_importance
+ 模型选择 Model Selection: 多试试不同模型，Voting/Stacking/Bagging(RF)/Boosting(GBDT)
+ 交叉验证 Cross Validation: K-Fold, 混淆矩阵(recall、precision、accuracy、AUC)

## 学习目标 / 损失函数 Loss / 调优指标
+ Objective=loss+penalty =（经验风险+结构风险）
+ 可参考 https://catboost.ai/en/docs/concepts/loss-functions-classification
```
- 0-1损失 : f(y,f) = 0 if y == f else 1
- 平方损失**MSE**:f(y,f) = power(y-f, 2)   # MSE, mean squared error 
- 绝对损失:f(y,f) = abs(y-f)
- 对数损失:f(y,P) = - logP(Y|X) 
```
```
正则化（regularization）：添加结构风险项penalty（剃须刀原则）:
L1(Lasso)：稀疏性，更能特征选择
L2(Ridge)：平滑性
L1+L2(ElasticNet)
剪枝：决策树
EarlyStoping
Batch-Normalization:批输入数据
Dropout
```

调优指标
- 混淆矩阵（confusion matirx）：FP，TP（True Positive），TN，FN
- 正确率 accuracy  = 1- error / total
- 精确率 precision = TP/(TP+FP)
- 召回率 recall = TP/(TP+FN)
- 调和均值：1/F1 = 1/2(1/P+1/R)  # 还有带beta的调合均值
- 多分类下：宏观精确率，微观精确率。。。
- ROC、AUC：比较适合看Ranking的质量。gAUC，分组看AUC

## 参数(凸)优化
+ 下降法(一阶梯度)
+ 牛顿法(二阶梯度，更快)
+ 加动量、加回退机制等
+ 网格搜索法



## 线性模型
+ 线性模型的优点是简单易实现，缺点是仅适用于线性可分（或可拟合）数据，LR容易欠拟合。
+ 最小二乘法
```
- 目标式Objective:平方损失 Loss=power(y-f,2)，即MSE
- fx = wx+b    # w,x是相同维度向量
- 代入目标式，取极小值（另w和b的偏导都为0），可得闭式解。（因此是非常快的）。 
```
+ Lasso: Objective = MSE + a * L1
+ Rigde 岭回归: Objective = MSE + b * L2 
+ ElasticNet: Objective = MSE + a * L1 + b * L2
+ Logistic Regression(LR, 逻辑回归)
```
- 实际是线性分类模型
- S型(Sigmoid)，非负性，对称性。在中间具有大的区分性
- 可以用 参数估计 + 最大熵原理(在所有可能的概率模型中，熵最大的模型是最大概率可能的模型) 求解参数
- 容易欠拟合
```
+ Perceptron 感知机：SVM要求结构风险最小，而感知机只要（找超平面wx=0）划分两类就可以了。python实现 https://github.com/doglex/ALGs/blob/master/MachineLearning/perceptron.py

## SVM
1.特点

- 间隔最大（结构风险最小）
- 核技巧（Kernel trick，内积引入对偶形式非常方便，可以将非线性问题转为线性问题）
- 损失：等价于Hinge Loss 合页损失
- 优化：凸二次规化，快速算法SMO（利用启发式下降）

2.优点：
- 存储小（只需要存储支持向量）
- 适用于线性和非线性
- 性能好（引入核被认为和一层神经网络的隐藏层）
- 适用与分类和回归（SVR）。

3.缺点：需要手工选取核（根据经验），原始的算法只能二分类。


4.类型：
- 线性可分SVM，硬间隔最大化：SVM = Perceptron+结构风险最小
- 不完全线性可分（在近处有少量Outlier），可引入松弛变量和惩罚因子。软间隔最大化
- SMO的python实现：https://github.com/doglex/ALGs/blob/master/MachineLearning/SVM/smo.py

## k近邻 kNN
1.思路：近朱者赤，近墨者黑。（其实仍然是一种基于相似性的协同过滤）

2.三个问题：

- 距离选择：欧式距离（Lp距离的第二范式）、马氏距离、Jaccard距离、余弦相似度、皮尔逊相关系数等。
- k值选择：k越小，模型复杂度越大，越容易过拟合。k一般不超过20，k=1时是最近邻算法。
- 分类决策：0-1损失下的经验风险最小化等价于多数表决。（还是需要假设一下数据的均匀分布的吧）

3.**免训练模型**，意味着直接判决。为了加速索引，使用kd-tree和ball-tree来减小搜索范围。

4.python代码实现：https://github.com/doglex/ALGs/blob/master/MachineLearning/kNN.py


## 朴素贝叶斯 Naive Bayes
1.贝叶斯思想：**后验 = 似然 x 先验**  

2.**为何朴素：求解条件概率(似然)时假设各个属性独立。**这是一个很强的假设。

3.朴素贝叶斯模型（研究Y）
- 先验分布P(Y)
- 后验分布P(Y|X)
- 由联合概率推导 P(X,Y) = P(X) P(Y|X) = P(Y) P(X|Y)  // 贝叶斯公式
- 后验概率  P(Y|X) =  P(Y) P(X|Y) / P(X)    
```
  其中P(Y)是先验，后面相乘是似然
  其中分母P(X)在一个特定的x下讨论各个y时是一致的因此可以忽略
  其中条件概率可以利用朴素（独立假设）来分解 multiply(P(Xi|Y))，而每一个因子的求解在最大似然假设下估计为数量占比（即在y下取得某个x的概率，用频率来估计他）
```
- 贝叶斯模型  argmax <y for all> P(Y|X = vector_x)  = argmax <y for all> P(Y) P(X = vector_x|Y)

4.在0-1损失下最大化后验概率相当于经验风险最小化。

5.平滑
- 为什么要平滑？因为很多0，直接相乘，乘出0了
- 拉普拉斯平滑：给这个feature的各种取值情况添加1或者一个较小值。还有一些其他平滑方法。

6.python实现：https://github.com/doglex/ALGs/blob/master/MachineLearning/naive_bayes/naive_bayes.py



## (多叉)决策树  Decision Tree
### ID3  信息增益(最大进行一次分裂，消耗一个特征)
- 香农熵 H(X) = - sum(p x logp)
- 条件熵 H(Y|X) = sum(p x H(Yi|X=Xi))
- 信息增益 g(D,A) = H(D) - H(D|A) = H(D) - |Di| / |D| x H(Di | Ai)   >= 0     // argue feature Ai
> 注：信息增益也称互信息，若X，Y独立，那么H(Y) = H(Y|X) ，互信息为0，否则H(Y) > H(Y|X).

### C4.5  信息增益比
- 以信息增益划分将严重偏向于选择取值较多的特征（比如id那一列），需要减小取值量的影响。
- r(D,A) = g(D,A) / H(splits) ,H(splits) = sum(|Di|/|D|log(|Di|/|D|))

### **CART** (classification and regression trees) 基尼指数(Gini Index)
- XGBoost就是用这种作为基础模型
- Gini (D) = 1- sum(power(p,2))   
- 基尼指数数值上可以近似于熵，且计算简单。基尼指数本身可以表示可能的误差程度（不确定程度）。

### 分裂终止条件
- 所有实例是同一类。（或者置信度足够，这类在这个结点中占绝大多数。或者说基尼指数足够小。）
- 没有更多的特征用来区分。（每一次分裂都需要消耗掉一个特征）
- (optional)信息增益小于阈值。（分裂带来的增益很小，却需要增大一个复杂度）
- (optional)达到最大深度。（几乎不用计算的指标）
- (optional)这个节点的样本数小于预定值。（比如用于CART回归树中）

### 决策树剪枝 
- 分为预剪枝和后剪枝
- 本质是简化模型，减少过拟合情况
- 随机森林等集成模型不必剪枝，因为本身有行采样+列采样，引入了随机性，不易过拟合


## 集成方法
> 集成方法：多个弱分类器组合成强分类器。

> 集成的前提是弱分类器不至于太差。

### 几种集成技术
+ Voting:对第一层的结果，等权投票就是Averaging，不等权可以通过网格搜索权重
+ Stacking(第一层多个模型输出做特征，搞第二层模型)
+ Bagging: 随机森林(Random Forest)的行采样+列采样，构造多棵树，降低variance(方差)。行采样就是随机抽取训练数据，列采样就是随机抽取特征。未被抽的训练数据称为OOB(out of bag)，可以用来加随机噪声干扰从而确定特征重要性。bagging易并行计算。
+ Boosting：GBDT通过对误分类的数据点调大误差权重，来迭代降低bias(偏差)。无法并行，对异常点敏感。
+ XGBoost: 结合RF和GBDT的优点，可以并行，使用CART做基分类器，添加L1和L2惩罚项。
+ LightGBM：微软开发的XGBoost
+ CatBoost：Yandex开发的XGBoost，cat是category的意思，对离散特征不用手动做onehot，非常方便使用，也支持GPU，