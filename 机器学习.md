## 类型
- **监督学习**（surpervised learning）：有X,y; 分类（Classification离散目标）、回归（Regression拟合连续目标）、排名（Ranking）
- **非监督学习**（unsurpervised learning）：有X无y；聚类（Clustering）、去噪（自编码器Auto Encoding）
- **半监督学习**（semi-surpervised learning）：打了一部分标签
- **增强学习**（reinforcement learning）：agent和环境的不断交互，进行奖励
> 监督不监督的区别在于是否是哟label

> 分类和回归的区别在于是否枚举值(离散值)

> 生成模型(先要算一遍联合概率)有：Naive Bayes，HMM； 判别模型有：Perceptron，KNN，DT，LR，SVM，emsemble，NN（神经网）,CRF(条件场)。

## 学习步骤
+ 数据获取与清洗 Data Cleaning: 升维，ETL
+ 特征提取 Feature Extraction: 升维，缺失值填充，归一化(Z-score/Min-Max Normalization)
+ 特征选择 Feature Selection: 降维，线性相关(皮尔逊系数)，PCA，model.feature_importance
+ 模型选择 Model Selection: 多试试不同模型，Voting/Stacking/Bagging(RF)/Boosting(GBDT)
+ 交叉验证 Cross Validation: K-Fold, 混淆矩阵(recall、precision、accuracy、AUC)

## 学习目标 / 损失函数 Loss / 调优指标
+ Objective=loss+penalty =（经验风险+结构风险）
+ 可参考 https://catboost.ai/en/docs/concepts/loss-functions-classification
```
- 0-1损失 : f(y,f) = 0 if y == f else 1
- 平方损失**MSE**:f(y,f) = power(y-f, 2)   # MSE, mean squared error 
- 绝对损失:f(y,f) = abs(y-f)
- 对数损失:f(y,P) = - logP(Y|X) 
```
```
正则化（regularization）：添加结构风险项penalty（剃须刀原则）:
L1(Lasso)：稀疏性，更能特征选择
L2(Ridge)：平滑性
L1+L2(ElasticNet)
剪枝：决策树
EarlyStoping
Batch-Normalization:批输入数据
Dropout
```

调优指标
- 混淆矩阵（confusion matirx）：FP，TP（True Positive），TN，FN
- 正确率 accuracy  = 1- error / total
- 精确率 precision = TP/(TP+FP)
- 召回率 recall = TP/(TP+FN)
- 调和均值：1/F1 = 1/2(1/P+1/R)  # 还有带beta的调合均值
- 多分类下：宏观精确率，微观精确率。。。
- ROC、AUC：比较适合看Ranking的质量。gAUC，分组看AUC

## 参数(凸)优化
+ 下降法(一阶梯度)
+ 牛顿法(二阶梯度，更快)
+ 加动量、加回退机制等
+ 网格搜索法



## 线性模型
+ 线性模型的优点是简单易实现，缺点是仅适用于线性可分（或可拟合）数据，LR容易欠拟合。
+ 最小二乘法
```
- 目标式Objective:平方损失 Loss=power(y-f,2)，即MSE
- fx = wx+b    # w,x是相同维度向量
- 代入目标式，取极小值（另w和b的偏导都为0），可得闭式解。（因此是非常快的）。 
```
+ Lasso: Objective = MSE + a * L1
+ Rigde 岭回归: Objective = MSE + b * L2 
+ ElasticNet: Objective = MSE + a * L1 + b * L2
+ Logistic Regression(LR, 逻辑回归)
```
- 实际是线性分类模型
- S型(Sigmoid)，非负性，对称性。在中间具有大的区分性
- 可以用 参数估计 + 最大熵原理(在所有可能的概率模型中，熵最大的模型是最大概率可能的模型) 求解参数
- 容易欠拟合
```
+ Perceptron 感知机：SVM要求结构风险最小，而感知机只要（找超平面wx=0）划分两类就可以了。python实现 https://github.com/doglex/ALGs/blob/master/MachineLearning/perceptron.py

## SVM
1.特点

- 间隔最大（结构风险最小）
- 核技巧（Kernel trick，内积引入对偶形式非常方便，可以将非线性问题转为线性问题）
- 损失：等价于Hinge Loss 合页损失
- 优化：凸二次规化，快速算法SMO（利用启发式下降）

2.优点：
- 存储小（只需要存储支持向量）
- 适用于线性和非线性
- 性能好（引入核被认为和一层神经网络的隐藏层）
- 适用与分类和回归（SVR）。

3.缺点：需要手工选取核（根据经验），原始的算法只能二分类。

4. 类型：
- 线性可分SVM，硬间隔最大化：SVM = Perceptron+结构风险最小
- 不完全线性可分（在近处有少量Outlier），可引入松弛变量和惩罚因子。软间隔最大化
- SMO的python实现：https://github.com/doglex/ALGs/blob/master/MachineLearning/SVM/smo.py

